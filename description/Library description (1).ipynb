{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b581e43d",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "Pandas is an open source Python package that is most widely used for data analysis\n",
    "Pandas makes it simple to do many of the time consuming, repetitive tasks associated with working with data, including:<br>\n",
    "- Data cleansing <br>\n",
    "- Data fill <br>\n",
    "- Data normalization <br>\n",
    "- Merges and joins <br>\n",
    "- Data visualization <br>\n",
    "- Statistical analysis <br>\n",
    "- Data inspection <br>\n",
    "- Loading and saving data<br>\n",
    "- Lable encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89e2d9a",
   "metadata": {},
   "source": [
    "## Numpy\n",
    "- NumPy is a Python library used for working with arrays.<br>\n",
    "- NumPy stands for Numerical Python.<br>\n",
    "- In Python we have lists that serve the purpose of arrays, but they are slow to process.<br>\n",
    "- NumPy aims to provide an array object that is up to 50x faster than traditional Python lists.<br>\n",
    "- NumPy arrays are stored at one continuous place in memory unlike lists, so processes can access and manipulate them very efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd3929",
   "metadata": {},
   "source": [
    "## Matplotlib\n",
    "- Matplotlib is a visualization library in Python for 2D plots of arrays. Matplotlib is a multi-platform data visualization library built on NumPy arrays and designed to work with the broader SciPy stack<br>\n",
    "- It allows us visual access to huge amounts of data in easily digestible visuals. Matplotlib consists of several plots and charts  like line, bar, scatter, histogram,etc.., pie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b90ccd",
   "metadata": {},
   "source": [
    "## Seaborn\n",
    "- Seaborn is a library for making statistical graphics in Python. It builds on top of matplotlib and integrates closely with pandas data structures.<br>\n",
    "- Seaborn helps to explore and understand the data. Its plotting functions operate on dataframes and arrays containing whole datasets and internally perform the necessary semantic mapping and statistical aggregation to produce informative plots. Its dataset-oriented, declarative API lets you focus on what the different elements of your plots mean, rather than on the details of how to draw them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9900472",
   "metadata": {},
   "source": [
    "## Train test split\n",
    "- The train-test split is a technique for evaluating the performance of a machine learning algorithm.<br>\n",
    "- It can be used for classification or regression problems and can be used for any supervised learning algorithm.\n",
    "- The procedure involves taking a dataset and dividing it into two subsets. The first subset is used to fit the model and is referred to as the training dataset. The second subset is not used to train the model; instead, the input element of the dataset is provided to the model, then predictions are made and compared to the expected values. This second dataset is referred to as the test dataset.<br>\n",
    "- Train Dataset: Used to fit the machine learning model.<br>\n",
    "- Test Dataset: Used to evaluate the fit machine learning model.<br>\n",
    "- The procedure has one main configuration parameter, which is the size of the train and test sets. This is most commonly expressed as a percentage between 0 and 1 for either the train or test datasets. For example, a training set with the size of 0.67 (67 percent) means that the remainder percentage 0.33 (33 percent) is assigned to the test set.\n",
    "<br>\n",
    "Nevertheless, common split percentages include:\n",
    "<br>\n",
    "- Train: 80%, Test: 20%<br>\n",
    "- Train: 67%, Test: 33%<br>\n",
    "- Train: 50%, Test: 50%<br>\n",
    "The size of the split can be specified via the “test_size” argument that takes a number of rows (integer) or a percentage (float) of the size of the dataset between 0 and 1. <br>\n",
    "When comparing machine learning algorithms, it is desirable that they are fit and evaluated on the same subsets of the dataset.\n",
    "<br>\n",
    "This can be achieved by fixing the seed for the pseudo-random number generator used when splitting the dataset. If you are new to pseudo-random number generators\n",
    "<br>\n",
    "This can be achieved by setting the “random_state” to an integer value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430de03c",
   "metadata": {},
   "source": [
    "## Suppor Vector Machine\n",
    "- SVM is a supervised machine learning algorithm which can be used for classification or regression problems. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs. <br>\n",
    "- The benefit is that we can capture much more complex relationships between our datapoints without having to perform difficult transformations on our own.<br> \n",
    "- objective of a Linear SVC (Support Vector Classifier) is to fit to the data you provide, returning a \"best fit\" hyperplane that divides, or categorizes, your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d174b",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "- Logistic regression is a statistical method used to predict the outcome of a dependent variable based on previous observations. It's a type of regression analysis and is a commonly used algorithm for solving binary classification problems.<br>\n",
    "- Logistic regression works by measuring the relationship between the dependent variable (what we want to predict) and one or more independent variables (the features). It does this by estimating the probabilities with the help of its underlying logistic function.<br>\n",
    "- Logistic regression is applied to predict the categorical dependent variable.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0777691e",
   "metadata": {},
   "source": [
    "## DecisionTree Classifier\n",
    "Decision Tree is a tool used for classificaiton and prediction. A Decision tree is a flowchart like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label<br>\n",
    "### Decision Tree Representation: \n",
    "Decision trees classify instances by sorting them down the tree from the root to some leaf node, which provides the classification of the instance. An instance is classified by starting at the root node of the tree, testing the attribute specified by this node, then moving down the tree branch corresponding to the value of the attribute as shown in the above figure. This process is then repeated for the subtree rooted at the new node. \n",
    "#### Strengths of Decision Tress\n",
    "- Decision trees are able to generate understandable rules.<br>\n",
    "- Decision trees perform classification without requiring much computation.<br>\n",
    "- Decision trees are able to handle both continuous and categorical variables.<br>\n",
    "- Decision trees provide a clear indication of which fields are most important for prediction or classification.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3626c13a",
   "metadata": {},
   "source": [
    "## RandomForest Classifier\n",
    "Random Forest is a learning method that operates by constructing multiple decision trees. The final decision is made based on the majority of the trees and is chosen by the random forest.<br>\n",
    "A decision tree is a tree-shaped diagram used to determine a course of action. Each branch of the tree represents a possible decision, occurrence, or reaction.<br>\n",
    "There are a lot of benefits to using Random Forest Algorithm, but one of the main advantages is that it reduces the risk of overfitting and the required training time. Additionally, it offers a high level of accuracy. Random Forest algorithm runs efficiently in large databases and produces highly accurate predictions by estimating missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25b3f2f",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "Metrics are used to monitor and measure the performance of a model (during training and testing)\n",
    "### Accuracy score\n",
    "Accuracy is defined as the percentage of correct predictions for the test data. It can be calculated easily by dividing the number of correct predictions by the number of total predictions.\n",
    "### r2_score\n",
    "The R2 score is a very important metric that is used to evaluate the performance of a regression-based machine learning model. It is pronounced as R squared and is also known as the coefficient of determination. It works by measuring the amount of variance in the predictions explained by the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209181d6",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Data preprocessing in Machine Learning refers to the technique of preparing (cleaning and organizing) the raw data to make it suitable for a building and training Machine Learning models.\n",
    "### StandardScaler\n",
    "Standardize generally means changing the values so that the distribution’s standard deviation equals one.StandardScaler removes the mean and scales each feature/variable to unit variance.\n",
    "### LabelEncoding\n",
    "Label Encoding refers to converting the labels into a numeric form so as to convert them into the machine-readable form. Machine learning algorithms can then decide in a better way how those labels must be operated. It is an important pre-processing step for the structured dataset in supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a726f80",
   "metadata": {},
   "source": [
    "### Warnings\n",
    "To ignore warnings in the file we import warnings library so by this we can ignore the warnings in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d506d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
